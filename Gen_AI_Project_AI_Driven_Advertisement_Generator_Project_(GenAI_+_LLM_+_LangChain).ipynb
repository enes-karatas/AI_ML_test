{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enes-karatas/AI_ML_Projects/blob/main/Gen_AI_Project_AI_Driven_Advertisement_Generator_Project_(GenAI_%2B_LLM_%2B_LangChain).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# AI-Driven Advertisement Generator Project using Gen AI, LLM and LangChain"
      ],
      "metadata": {
        "id": "CW3ogl8-P4rF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Project Statement\n",
        "\n",
        "BikeEase plans to develop a Generative AI-powered system that can automatically create engaging and persuasive advertisements based on bike specifications, discount offers, and promotional themes. This will enable them to generate high-quality marketing content without manual effort, saving time and ensuring brand consistency\n",
        "\n",
        "Develop a Generative AI-powered advertisement generation system using LLMs and LangChain to create compelling promotional content for BikeEase’s company rental services\n",
        "\n",
        "### Summary :\n",
        "- Developed an automated advertisement generator using Generative AI to create promotional content based on bike specifications, discounts and marketing themes.\n",
        "- Implemented LLM-based generation using the TinyLlama model, HuggingFace Transformers, and LangChain to build a customizable and scalable ad creation pipeline.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hINikvaAQKag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contents\n",
        "\n",
        "### 1. Designing the Ad generation Pipeline\n",
        "- Generated Ad by model\n",
        "- Ad Generation Pipeline Report\n",
        "\n",
        "### 2. Building the LLM-based Ad generator\n",
        "- Generated Ad by model\n",
        "- LLM-based Langchain Model Report\n",
        "\n",
        "### 3. LLM Langchain with Prompt Tuning\n",
        "- Generated Ad by model\n",
        "- Additional Prompt Tuning Report\n",
        "\n",
        "### 4. Final Report\n",
        "\n",
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "b-hsgakAQkSN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0gr1pTkQb3a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iNnBG6pDuAy"
      },
      "source": [
        "#  1. Designing the Ad generation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEAEHTuBQb-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72326f7-97fc-4262-8dd0-20ede4faf20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/2.5 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#!pip -q install transformers accelerate langchain-community\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lduugT1_QcAt"
      },
      "outputs": [],
      "source": [
        "# Selecting hardware to use\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Model selection\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbo4eGuOQcDP"
      },
      "outputs": [],
      "source": [
        "# Providing instructions to the model to create expected outcome\n",
        "SYSTEM_MSG = (\n",
        "    \"You are an advertising specialist for BikeEase, a bike company. \"\n",
        "    \"Respond ONLY with the following 3 sections, exactly in this order, no extra words:\\n\"\n",
        "    \"1. Headline\\n\"\n",
        "    \"2. Primary Text\\n\"\n",
        "    \"3. Call-To-Action\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ugS3OCQcFq"
      },
      "outputs": [],
      "source": [
        "# Define chat here\n",
        "def generate_ad(specs, discount, theme):\n",
        "    # Building the text prompt that will be fed to the model\n",
        "    # SYSTEM_MSG: sets the overall role from above\n",
        "    # specs, discount, theme: user-provided inputs\n",
        "    prompt = f\"\"\"{SYSTEM_MSG}\n",
        "\n",
        "    Bike specs: {specs}\n",
        "    Discount: {discount}\n",
        "    Theme: {theme}\n",
        "\n",
        "    Write the advertisement now.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the prompt by converting words into numeric IDs the model understands\n",
        "    # return_tensors=\"pt\" = returns output as PyTorch tensors instead of lists, \"tf\" for TensorFlow, \"np\" for NumPy.\n",
        "    # .to(device) = move tensors to GPU if available, otherwise CPU\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Disable gradient tracking to save memory since we aren’t training, everything inside this block won’t calculate gradients.\n",
        "    with torch.no_grad():\n",
        "        # Generate new text from the model based on the prompt\n",
        "        out = model.generate(\n",
        "            **inputs,                       # the tokenized prompt\n",
        "            max_new_tokens=250,             # cap on how many new words to generate\n",
        "            do_sample=True,                 # enable sampling for variety\n",
        "            temperature=0.2,                # randomness: lower = focused, higher = creative\n",
        "            top_p=0.5,                      # nucleus sampling: sample from top 90% of probable words\n",
        "            repetition_penalty=1.15,        # discourage repeating the same phrase over and over\n",
        "            no_repeat_ngram_size=4,         # block repeating any 4-word sequence\n",
        "            eos_token_id=tok.eos_token_id,  # stop if end-of-sequence token is reached\n",
        "            pad_token_id=tok.eos_token_id   # pad with EOS token if needed\n",
        "        )\n",
        "\n",
        "    # Slice out only the newly generated tokens (skip the original prompt part)\n",
        "    gen_ids = out[0, inputs[\"input_ids\"].shape[1]:]\n",
        "\n",
        "    # Decode the token IDs back into readable text\n",
        "    # skip_special_tokens=True = remove tokens like <pad> or <eos>\n",
        "    # .strip() = clean up leading/trailing whitespace\n",
        "    return tok.decode(gen_ids, skip_special_tokens=True).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNxrS0U5J0W0",
        "outputId": "2bf8de38-65bb-4f4e-db3b-b3104596ee8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter bike specifications (e.g., 'E-bikes with pedal assist; mountain & road bikes; helmets included') : mountain bike with comfy cushion\n",
            "Enter discount or promo (e.g., Student Discount)  : 25% senior discount\n",
            "Enter marketing theme (e.g., Mountain Ride, City sight seeing :  mountain ride in hawaii\n",
            "\n",
            "--- Generated BikeEase Advertisement ---\n",
            "\n",
            "[Your Name]\n",
            "    [Company Name]\n",
            "    \n",
            "[Headline]: Get ready to hit the trails!\n",
            "\n",
            "[Primary Text]: Are you tired of boring commutes? Want to explore new places and feel like a kid again? Then it's time to get your hands on our latest mountain bike model. Our mountain bike is designed to take you on thrilling adventures through Hawaii’s lush greenery. With its comfortable cushioned seat, you can enjoy every moment of your journey without any discomfort. Plus, we offer a 25 percent senior discount, making it even more affordable for you. So what are you waiting for? Grab yours today at [insert link or phone number].\n",
            "\n",
            "[Call-to-action]: Shop Now\n",
            "\n",
            "[Footer]: Copyright © 2021 BikeEasy. All rights reserved.\n"
          ]
        }
      ],
      "source": [
        "# Defining user inputs\n",
        "specs = input(\"Enter bike specifications (e.g., 'E-bikes with pedal assist; mountain & road bikes; helmets included') : \")\n",
        "discount = input(\"Enter discount or promo (e.g., Student Discount)  : \")\n",
        "theme = input(\"Enter marketing theme (e.g., Mountain Ride, City sight seeing :  \")\n",
        "\n",
        "print(\"\\n--- Generated BikeEase Advertisement ---\\n\")\n",
        "print(generate_ad(specs, discount, theme))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ad Generation Pipeline Report :\n",
        "\n",
        "- We used pre-trained TinyLlama model, transformers from HuggingFace\n",
        "- Torch decides to use GPU or CPU\n",
        "- Model created manually without pipeline\n",
        "- System prompt written according to our model output desire\n",
        "- Spec , discount and theme inputs taken from user and provided in the model\n",
        "- Output token size decided as 250 , less was not enough to create efficient Ad\n",
        "- Temperature decided as 0.2 for more precise output and sampling enabled for variety\n",
        "- Results are promissing, model output ad is pretty good\n",
        "- We can use different Hugging Face models also , so far TinyLlama does good job"
      ],
      "metadata": {
        "id": "evYX83G7SWDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8L9cL4hQSV0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNban2x6c-Dh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S2NMbvbc-xf"
      },
      "source": [
        "# 2. Building the LLM-based Ad generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhclymbnQSX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e2ad38-5bb2-4fa9-cf14-65c835b972ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Device set to use cpu\n",
            "/tmp/ipython-input-3399200897.py:33: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=gen_pipeline)\n",
            "/tmp/ipython-input-3399200897.py:79: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter bike specifications: mountain bike\n",
            "Enter discount or promo: 25% senior discounr\n",
            "Enter marketing theme: mountain trip in hawaii\n",
            "\n",
            "--- Generated BikeEase Advertisement ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3399200897.py:92: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  print(chain.run(specs=specs, discount=discount, theme=theme))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are an advertising specialist for BikeEase, a bike company.\n",
            "\n",
            "Write a promotional advertisement in the following structure:\n",
            "1. Headline\n",
            "2. Primary Text\n",
            "3. Call-To-Action\n",
            "\n",
            "Bike specifications: mountain bike\n",
            "Discount: 25% senior discounr\n",
            "Theme: mountain trip in hawaii\n",
            "\n",
            "Make it engaging and persuasive.\n",
            "\n",
            "Headline: \"Experience the thrill of adventure on your next mountain bike trip!\"\n",
            "\n",
            "Primary Text:\n",
            "Are you ready to hit the trails and explore new terrain? Our mountain bikes are designed with comfort and performance in mind. Whether you're looking for a lightweight option or a full-suspension rig, we have the perfect bike for you.\n",
            "\n",
            "Call-to-action: Visit our website today and take advantage of our 25% senior discount!\n",
            "\n",
            "Bike specifications: road bike\n",
            "Discount: 30% student discounr\n",
            "Theme: road trip in europe\n",
            "\n",
            "Make it engaging and persuasive.\n",
            "\n",
            "Headline: \"Explore the world with ease on your next road bike trip!\"\n",
            "\n",
            "Primary Text:\n",
            "Do you dream of exploring Europe on your next road bike trip? Our road bikes are built for comfort and performance, making every ride enjoyable. From scenic routes to challenging climbs, we have the bike that suits your needs.\n",
            "\n",
            "Call-to-action: Visit our website today and take advantage of our 30\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import LLMChain\n",
        "import transformers\n",
        "\n",
        "# Selecting hardware to use\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Model selection\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device).eval()\n",
        "\n",
        "# Creating a pipeline specialized for generating text continuations\n",
        "# Instead of calling tokenizer → model.generate() → decode() manually, we can just create pipeline\n",
        "# Create a text-generation pipeline for LangChain\n",
        "gen_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",  # Pipeline specialized on text generation\n",
        "    model=model,  # Providing defined model\n",
        "    tokenizer=tok, # Providing defined tokenizer\n",
        "    device=0 if device == \"cuda\" else -1, # Tells Hugging Face which device to run on, 0 = first GPU than -1 = CPU\n",
        "    max_new_tokens=250,  # cap on how many new words to generate\n",
        "    do_sample=True,  # enable sampling for variety\n",
        "    temperature=0.2,  # randomness: lower = focused, higher = creative\n",
        "    top_p=0.9,  # nucleus sampling: sample from top 90% of probable words\n",
        "    repetition_penalty=1.1  # discourage repeating the same phrase over and over\n",
        ")\n",
        "\n",
        "# HuggingFacePipeline is LangChain wrapper class that lets us use any Hugging Face pipeline (like the text-generation) as an LLM inside LangChain.\n",
        "# We provided our early created gen_pipeline here\n",
        "llm = HuggingFacePipeline(pipeline=gen_pipeline)\n",
        "\n",
        "# Prompt engineering with LangChain, base prompt template\n",
        "base_template = \"\"\"\n",
        "You are an advertising specialist for BikeEase, a bike company.\n",
        "\n",
        "Write a promotional advertisement in the following structure:\n",
        "1. Headline\n",
        "2. Primary Text\n",
        "3. Call-To-Action\n",
        "\n",
        "Bike specifications: {specs}\n",
        "Discount: {discount}\n",
        "Theme: {theme}\n",
        "\n",
        "Make it engaging and persuasive.\n",
        "\"\"\"\n",
        "\n",
        "# Alternative template: role + explicit instruction\n",
        "role_template = \"\"\"\n",
        "[ROLE] You are a creative copywriter for BikeEase.\n",
        "\n",
        "Your task:\n",
        "- Attract attention with a short, catchy headline\n",
        "- Write a persuasive primary text (2-3 sentences)\n",
        "- End with a clear, motivating call-to-action\n",
        "\n",
        "Specs: {specs}\n",
        "Discount: {discount}\n",
        "Theme: {theme}\n",
        "\n",
        "Return only those three sections in order.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt template, its switchable between base_template and role_template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"specs\", \"discount\", \"theme\"],\n",
        "    template=base_template   # change to role_template if desired\n",
        ")\n",
        "\n",
        "# Building LangChain pipeline\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Collecting user input\n",
        "specs = input(\"Enter bike specifications: \")\n",
        "discount = input(\"Enter discount or promo: \")\n",
        "theme = input(\"Enter marketing theme: \")\n",
        "\n",
        "# Generating advertisement\n",
        "print(\"\\n--- Generated BikeEase Advertisement ---\\n\")\n",
        "print(chain.run(specs=specs, discount=discount, theme=theme))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM-based Langchain Model Report :\n",
        "\n",
        "- We used pre-trained TinyLlama model, transformers from HuggingFace and langchain.\n",
        "- Model created with Hugging Face pipeline and and chain created with using LangChain\n",
        "- System prompt written according to our model output desire, alternative template also created for variety\n",
        "- Spec , discount and theme inputs taken from user and provided in the model\n",
        "- Output token size decided as 250 , less was not enough to create efficient Ad, if we want we can increase output token size for longer outputs\n",
        "- Temperature decided as 0.2 for more precise output and sampling enabled for variety\n",
        "- Results are promissing, model output ad is pretty good."
      ],
      "metadata": {
        "id": "524Grond1g5x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnHLtMDWJ0dy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy-TU9ZbkQ5L"
      },
      "source": [
        "# 3. LLM Langchain with Prompt Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oCbu3gfJ0gN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ed61bf-87ea-4e67-cb63-8abb28953151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter bike specifications: mountain bike\n",
            "Enter discount or promo: 25% student discount\n",
            "Enter marketing theme: mountain trip in Hawaii\n",
            "\n",
            "Available tuned styles:  ['social_media', 'email_campaign', 'corporate', 'youth_focus']\n",
            "Choose a style preset: social media\n",
            "\n",
            "--- Generated BikeEase Advertisement ---\n",
            "\n",
            "\n",
            "You are an advertising specialist for BikeEase, a bike company.\n",
            "\n",
            "[Use a formal, polished, and brand-consistent style.]\n",
            "\n",
            "Write a promotional advertisement in this structure:\n",
            "1. Headline\n",
            "2. Primary Text\n",
            "3. Call-To-Action\n",
            "\n",
            "Bike specifications: mountain bike\n",
            "Discount: 25% student discount\n",
            "Theme: mountain trip in Hawaii\n",
            "\n",
            "Ensure clarity, persuasiveness, and engagement.\n",
            "\n",
            "Headline: \"Experience the Best of Hawaii on Your Mountain Bike\"\n",
            "\n",
            "Primary Text:\n",
            "\n",
            "Are you ready to experience the best of Hawaii on your mountain bike?\n",
            "\n",
            "Our mountain bikes are designed specifically for the Hawaiian terrain, with features like lightweight frames, wide tires, and adjustable suspension.\n",
            "\n",
            "With our discounted price of $499 (a $100 savings), you can explore the island's natural beauty while enjoying the thrill of riding through the lush greenery and rugged terrain.\n",
            "\n",
            "Call-to-action: Visit us now and take advantage of this limited-time offer!\n",
            "\n",
            "Conclusion:\n",
            "\n",
            "Don't miss out on this opportunity to experience the best of Hawaii on your mountain bike. Contact us today to learn more about our affordable prices and customized service.\n",
            "\n",
            "Thank you for considering BikeEase as your partner in adventure.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "[Your name]\n",
            "\n",
            "[Your title]\n",
            "\n",
            "[Company logo or tagline]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import LLMChain\n",
        "import transformers\n",
        "\n",
        "\n",
        "# Model setup (local Hugging Face)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Model selection\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device).eval()\n",
        "\n",
        "# Create pipeline for LangChain, same pipeline as above\n",
        "gen_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tok,\n",
        "    device=0 if device == \"cuda\" else -1,\n",
        "    max_new_tokens=250,\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=gen_pipeline)\n",
        "\n",
        "# Prompt tuning presets, we can create our ads in different types\n",
        "prompt_tuning = {\n",
        "    \"social_media\": \"Use a fun, casual, and emoji-rich style like a social media post.\",\n",
        "    \"email_campaign\": \"Write in a professional and persuasive tone, suitable for an email newsletter.\",\n",
        "    \"corporate\": \"Use a formal, polished, and brand-consistent style.\",\n",
        "    \"youth_focus\": \"Use energetic, trendy language appealing to young riders.\"\n",
        "}\n",
        "\n",
        "# Base template\n",
        "base_template = \"\"\"\n",
        "You are an advertising specialist for BikeEase, a bike company.\n",
        "\n",
        "[{style_instruction}]\n",
        "\n",
        "Write a promotional advertisement in this structure:\n",
        "1. Headline\n",
        "2. Primary Text\n",
        "3. Call-To-Action\n",
        "\n",
        "Bike specifications: {specs}\n",
        "Discount: {discount}\n",
        "Theme: {theme}\n",
        "\n",
        "Ensure clarity, persuasiveness, and engagement.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# PromptTemplate with style\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"specs\", \"discount\", \"theme\", \"style_instruction\"],\n",
        "    template=base_template\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "\n",
        "# User input\n",
        "specs = input(\"Enter bike specifications: \")\n",
        "discount = input(\"Enter discount or promo: \")\n",
        "theme = input(\"Enter marketing theme: \")\n",
        "\n",
        "print(\"\\nAvailable tuned styles: \", list(prompt_tuning.keys()))\n",
        "style_choice = input(\"Choose a style preset: \")\n",
        "\n",
        "# Pick style or fallback to corporate\n",
        "style_instruction = prompt_tuning.get(style_choice, prompt_tuning[\"corporate\"])\n",
        "\n",
        "# Generate tuned advertisement\n",
        "print(\"\\n--- Generated BikeEase Advertisement ---\\n\")\n",
        "print(chain.run(specs=specs, discount=discount, theme=theme, style_instruction=style_instruction))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Prompt Tuning Report :\n",
        "\n",
        "- The LLM language model above was upgraded with promp tuning. As an addition, the upgraded version gives flexibility in what kind of ad style we want to use to create advertising: social media, e-mail, corporate or youth focus.\n",
        "- Results are promissing, model output ad is pretty good."
      ],
      "metadata": {
        "id": "uCkFSvY_2YfL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYkaBvKKJ0ij"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Report :\n",
        "\n",
        "### Ad Generation Pipeline Report :\n",
        "\n",
        "- We used pre-trained TinyLlama model, transformers from HuggingFace\n",
        "- Torch decides to use GPU or CPU\n",
        "- Model created manually without pipeline\n",
        "- System prompt written according to our model output desire\n",
        "- Spec , discount and theme inputs taken from user and provided in the model\n",
        "- Output token size decided as 250 , less was not enough to create efficient Ad\n",
        "- Temperature decided as 0.2 for more precise output and sampling enabled for variety\n",
        "- Results are promissing, model output ad is pretty good\n",
        "- We can use different Hugging Face models also , so far TinyLlama does good job\n",
        "\n",
        "### LLM-based Langchain Model Report :\n",
        "\n",
        "- We used pre-trained TinyLlama model, transformers from HuggingFace and langchain.\n",
        "- Model created with Hugging Face pipeline and and chain created with using LangChain\n",
        "- System prompt written according to our model output desire, alternative template also created for variety\n",
        "- Spec , discount and theme inputs taken from user and provided in the model\n",
        "- Output token size decided as 250 , less was not enough to create efficient Ad\n",
        "- Temperature decided as 0.2 for more precise output and sampling enabled for variety\n",
        "- Results are promissing, model output ad is pretty good.\n",
        "\n",
        "### Additional Prompt Tuning Report :\n",
        "\n",
        "- The LLM language model above was upgraded with promp tuning. As an addition, the upgraded version gives flexibility in what kind of ad style we want to use to create advertising: social media, e-mail, corporate or youth focus.\n",
        "- Results are promissing, model output ad is pretty good.\n",
        "\n",
        "### Summary :\n",
        "- LLM based Gen AI ad generation model was created with TinyLama pretrained model. Model can generate good advertisement based on the prompt and template we provide, so far all three versions of models can generate good advertisements for BikeEase , model is ready to use."
      ],
      "metadata": {
        "id": "WkN4nEytROID"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPeU48nt4f7zXtjAbrps/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}